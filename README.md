# LMS-SPA

ProTranslating portal project using NodeJS ~20.9.0 and VueJS 2. LTS version of Ubuntu 20.04.

# Project structure

```
lms-spa/
├── app/
│   ├── components/ (A set of modules that the application uses to start and serve content)
│   │   ├── api-response/ (Handles known application errors and catches unhandled errors as well. Provides a helper method which defines and serves the proper response to the client)
│   │   ├── application/ (The application component initializes all the routing, swagger and security logic)
│   │   ├── audit/ (The audit component contains a middleware that handles audit trail and stores all the modifications performed by a user)
│   │   ├── configuration/ (The application configuration module)
│   │   ├── database/ (A set of database components)
│   │   │   ├── mongo/ (The mongo database component. Contains the logic to connect to mongo)
│   │   │   │   ├── services/ (A set of services that provides helper methods to access known collections)
│   │   ├── log/ (Logs every request and provides an application logger that writes a rotating file)
│   │   ├── session/ (Utils and helpers to check roles and access grants)
│   │   ├── version/ (Component that retrieves and expose the application version. Unknown for developer and the git tag name for production)
│   ├── endpoints/ (The http endpoints of the application. The endpoints use components as well)
│   │   ├── auth/ (The endpoints folder should state the endpoint uri, in this case "/api/auth")
│   │   ├── [...]
│   ├── migrations/ (Contains the migrations javascript files that execute before starting the application)
├── frontend/ (The frontend application)
│   ├── build/ (Build scripts provided by vuejs. Configures webpack dev server and production builds)
│   ├── config/ (Environment configs for the frontend app)
│   ├── dist/ (This is where the productions files are generated. This folder is ignored since it's generated by the build script)
│   ├── src/ (The frontend src)
│   │   ├── assets/ (Files that are handled by webpack thus they can be used or imported in the fronted files)
│   │   ├── components/ (The frontend application components)
│   │   │   ├── header/ (The header component)
│   │   │   ├── home/ (Contains the private components that are visible from within the private part of the application)
│   │   │   ├── login/ (Contains all the login related components)
│   │   │   ├── side-bar/ (Contains all the side-bar related components)
│   │   ├── resources/ (A set of endpoints resources that provides an easy way to perform request to the endpoints)
│   │   ├── services/ (A set of classes that wraps a resource and provide convenient methods. Components should use a service not resources).
│   │   ├── stores/ (A "flux-like" approach to handle the applications state)
│   │   │   ├── modules/ (A state module)
│   │   │   │   ├── app/ (The application general state)
│   │   │   │   ├── home/ (The "logged in" application state)
│   │   │   │   ├── sideBar/ (The side bar state)
│   │   ├── styles/ (General style SASS files)
│   │   ├── utils/ (General JS utils files)
│   │   ├── app_global.scss (The global application styles applied to all components)
│   │   ├── app.js (Logic for the main component)
│   │   ├── app.scss (Scoped styles for the main component)
│   │   ├── main.js (Bootstraps the whole frontend application)
│   │   ├── routes.js (Defines all the frontend routes and lazy loading strategy)
│   ├── static/ (Files that should be on the frontend but are not handled by webpack)
│   ├── test/ (Frontend unit tests)
│   ├── index.html (The only html file of the frontend. It includes the proper scripts that bootstrap the whole frontend application. Production builds should use the dist/index.html)
│   ├── package.json (The package.json of the frontend)
│   ├── README.md (Readme file for the frontend project. CRITICAL information should be mentioned in this file as well)
├── log/ (where the log files are stored in development)
├── test/ (The backend application's test)
│   ├── adhoc (Adhoc test)
│   ├── unit (The backend application's unit test.)
├── package.json (The package.json of the backend)
├── README.md (Readme file for the backend project. You're currently reading this file)
├── backend-deploy.sh (A script file that copies only the backend files to given a directory as the first parameter)
├── frontend-deploy.sh (A script file that copies only the frontend files to given a directory as the first parameter)
```

# Developer Setup

## Environment variables

The applications needs the following environment variables:

- `NODE_ENV` = "DEV", "TEST", "production", "UNIT".
- `CRYPTO_KEY_PATH` = "key/key"
- `AUDIT_TRAIL_FULL_RESPONSE` = "false"
- `NODE_LOGS_PATH` = "logs" # or "" - sends logs to stdout
- `NODE_LOGS_FILTER` = ['password', 'creditCard', '']
- `NODE_LOGS_LEVEL` = info
- `MONGODB_LMS_CONNECTION_STRING` = "mongodb://user:password@127.0.0.1:27017/lms?ssl=true"
- `MONGODB_LMS_AUTH_CONNECTION_STRING` = "mongodb://user:password@127.0.0.1:27017/lms_auth?ssl=true"
- `MONGODB_LMS_AUDIT_CONNECTION_STRING` = "mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true"
- `HTTP_HOST`: "localhost",
- `HTTP_PORT`: "3443",
- `HTTPS_ON`: "false",
- `SESSION_TIMEOUT`: 30
- `LMS_BACKUP_AUDIT_PREFIX`: "/full/path/to/pts-lms-test-files/backup-audit-files"
- `LMS_BACKUP_AUDIT_CONNECTION_STRING` = "mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true"
- `LMS_BACKUP_AUDIT_COLLECTIONS` = audit_trails
- `LMS_BACKUP_NOTIFICATION_PREFIX`: "/full/path/to/pts-lms-test-files/backup-notification-files"
- `LMS_BACKUP_NOTIFICATION_CONNECTION_STRING` = "mongodb://user:password@127.0.0.1:27017/lms?ssl=true"
- `LMS_BACKUP_NOTIFICATION_COLLECTIONS` = notifications
- `LMS_FILES_PATH`: /full/path/to/uploads
- `LMS_KEY` : /opt/data/lms/certs/app.key"
- `LMS_CRT` : "/opt/data/lms/certs/app.crt"
- `AWS_S3_KEY`: ""
- `AWS_S3_SECRET`: ""
- `AWS_S3_BUCKET`: "pts-lms-local-files-firstname-lastname"
- `GCS_KEY_FILE`: ""
- `GCS_BUCKET`: "pts-lms-local-files-firstname-lastname"
- `USE_SECURE_COOKIES`: "false"
- `RAW_BODY_PAYLOAD_LIMIT`: "12mb"
- `S3_START_MULTIPART_COPY_AT_BYTES`: 52428800
- `ARCHIVE_FILES_IN_AWS`: true
- `ARCHIVE_FILES_IN_GCS`: true
- `PC_BASE_URL`: "http://localhost:8443"
- `MIGRATIONS_DISABLED`: false
- `SESSION_WITH_2FA_TIMEOUT`: 480
- `HOTP_MOCK_VALUE`: "123456"
- `ENTITY_NUMBER_STARTS_AT`: "See the section ENTITY NUMBERS RANGE ACROSS THE DEVS below"
- `E2E_OUT_FOLDER`: ""
- `CQ_MAX_EXECUTION_MINUTES`: 10
- `PWD_SALT_ROUND`: 12
- `LOG_AUDIT_TRAILS`: true
- `HEAP_SNAPSHOT_MEM_LIMIT_GB`: 1.5
- `HEAP_SNAPSHOT_MEM_LOWER_LIMIT_GB`: 1
- `HEAP_SNAPSHOT_MEM_UPPER_LIMIT_GB`: 2
- `MEMORY_CHECK_INTERVAL_MS`: 60000
- `HEAPDUMP_AFTER_TIME`: 00:00
- `HEAPDUMP_BEFORE_TIME`: 01:00
- `BILL_PROVIDER_TASKS_BATCH_SIZE`: 100
- `IMPORT_MODULE_ENTITIES`: Ability,AbilityExpenseAccount,ExpenseAccount,ActivityTag,AssignmentStatus,AuditTrails,BankAccount,BillingTerm,Breakdown,Certification,Company,CompanyDepartmentRelationship,CompanyMinimumCharge,CompetenceLevel,Connector,Currency,DeliveryMethod,DocumentType,Group,InternalDepartment,Lsp,Language,LeadSource,MtSetting,Location,Notification,Opportunity,PaymentMethod,SchedulingStatus,RequestType,Account,Scheduler,SoftwareRequirement,TaxForm,Template,CatTool,TranslationUnit,User,VendorMinimumCharge,BillAdjustment,Check,ApPayment,ArAdjustment,ArAdvance,ArInvoice,ArPayment,Activity,CompromisedPassword,LmsAuth,Country,State,Request
- `MIGRATIONS_DISABLED` disables migrations running when set to true

The variable `MEMORY_CHECK_INTERVAL_MS` Defines the interval in milliseconds at which the app tries to create heap snapshots.

The variable `HEAP_SNAPSHOT_MEM_LIMIT_GB` Defines the threshold in GB at which the app creates heap snapshots. The snapshots can be used for debugging memory leaks and are stored the /tmp/ directory

The variable `NODE_LOGS_FILTER` defines all the properties that should not be logged. For instance, if a user updates his creditCard number, that property will be filtered out from logs.

The kill switch flag `USE_SECURE_COOKIES` should be flag in your dev enviroment if you are working over HTTP.
`EOP_*` variables are part of integration stuff for a client. Anything can be set up in `EOP_*` if not working with that implementation.

`RAW_BODY_PAYLOAD_LIMIT` should be set to big enough value(>=50b). Otherwise server may return silent 500 with no log traces for any request.

`HTTPS_ON` is used to enable/disable https server. You might need to enable it to unlock some browser features which is working only in secure area (e.g. ServiceWorkers, navigator.clipboard)
`CRYPTO_KEY_PATH` points to a file that will be use as a synchronous key to encrypt and decrypt certain fields in the db. This file can be binary or plain text. More instructions on creating this directory and file will be provided later in this readme
`AWS_S3_BUCKET` The username inside the AWS CSV credentials files is the name of the AWS S3 and the GCP `GCS_BUCKET`. This will be provided by IT
`LMS_SECONDARY_CONNECTION_STRING` it is supposed to be the connection string for read only queries
and you set it with the same value as LMS_CONNECTION_STRING locally because locally you do not have an active-passive cluster

`PAYMENT_WRITE_PARALLEL_OPERATIONS_NUMBER` it is used for controlling the number of write operations when payments are created from the grid (not csv)

`AP_PAYMENT_TOTAL_NUMBER_OF_BATCH_ENTRIES` it is used for controlling the number of payment entries that are processed each time the ap-payment-poster scheduler runs

`AR_INVOICE_TOTAL_NUMBER_OF_BATCH_ENTRIES` it is used for controlling the number of invoice entries that are processed each time the ar-invoice-poster scheduler runs

`BILLS_TO_PROCESS_NUMBER_FOR_VENDOR_BALANCE_UPDATE` it is used for controlling the number of bills to be processed when updating vendor balances after creating bills

`BILL_PROVIDER_TASKS_BATCH_SIZE` it is used for controlling the number of provider tasks to process when
creating bills with bill-variable-rate scheduler

`HEAP_SNAPSHOT_MEM_LOWER_LIMIT_GB` it is used for controlling the lower limit at which heap snapshots are created

`HEAP_SNAPSHOT_MEM_UPPER_LIMIT_GB` it is used for controlling the upper limit at which heap snapshots are created

On MAC and Linux, environment variables can be setup in either ~/.bashrc or ~/.zshrc config files depending on your shell of choice.

Note: The step above assumes that your shell of choice is either bash or zsh. For users of shells other than these two, kindly refer to your shell's documentation to aid this process. For the rest of this document, it is assumed that your environment variables can be found in ~/.bashrc.

After modifying ~/.bashrc, run the following command to set the values.

```sh
source ~/.bashrc
```

For example:

```sh
export AUDIT_TRAIL_FULL_RESPONSE=false
export CRYPTO_KEY_PATH=/full/path/to/key-file
export NODE_LOGS_PATH=/var/log/lms/lms-node-log
export NODE_LOGS_FILTER=password,newPassword,currentPassword,repeatPassword,creditCard,emailDetails
export NODE_LOGS_LEVEL=info
export HTTP_HOST=0.0.0.0
export HTTP_PORT=3443
export HTTPS_ON=false
export SESSION_TIMEOUT=30
export USE_SECURE_COOKIES=
export RAW_BODY_PAYLOAD_LIMIT=12mb
export LMS_BACKUP_AUDIT_PREFIX=/full/path/to/pts-lms-test-files/backup-audit-files
export LMS_BACKUP_AUDIT_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true"
export LMS_BACKUP_AUDIT_COLLECTIONS=audit_trails
export LMS_BACKUP_NOTIFICATION_PATH=/full/path/to/pts-lms-test-files/backup-notification-files
export LMS_BACKUP_NOTIFICATION_PREFIX=/full/path/to/pts-lms-test-files/backup-notification-files
export LMS_BACKUP_NOTIFICATION_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms?ssl=true"
export LMS_BACKUP_NOTIFICATION_COLLECTIONS=notifications
export LMS_FILES_PATH=/full/path/to/uploads
export LMS_KEY=/opt/data/lms/certs/app.key
export LMS_CRT=/opt/data/lms/certs/app.crt
export MONGODB_LMS_CONNECTION_STRING=mongodb://user:password@127.0.0.1:27017/lms?ssl=true
export MONGODB_LMS_AUTH_CONNECTION_STRING=mongodb://user:password@127.0.0.1:27017/lms_auth?ssl=true
export MONGODB_LMS_AUDIT_CONNECTION_STRING=mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true
export AWS_S3_KEY=
export AWS_S3_SECRET=
export AWS_S3_BUCKET=
export GCS_KEY_FILE=
export GCS_BUCKET=
export PORTALMT_BASE_URL=
export HEAP_SNAPSHOT_MEM_LIMIT_GB=1.5
export MEMORY_CHECK_INTERVAL_MS=1
```

To get a list of all environment variables needed by the project:

```sh
cd ~/workspace/lms-spa
git grep "getenv" -- '*.js' | grep -o -E "[0-9A-Z_]{2,}" | sort -u
```

###############

## Setting up MongoDB

- Install [MongoDB version 5.0](https://www.mongodb.com/docs/v5.0/administration/install-community/)
- Check your mongo config contains the following. <br>
  MacOS: `/usr/local/etc/mongod.conf`<br>
  MacOS Apple Silicon via homebrew `/opt/homebrew/etc/mongod.conf`<br>
  Ubuntu: `/etc/mongod.conf`
  ```sh
  net:
    bindIp: 127.0.0.1 # 0.0.0.0 if you want this server to be accessed from anywhere
    port: 27017
  replication:
    replSetName: "rs0"
  ```
- restart the mongo service
  - In MacOS:
  ```sh
  brew services restart mongodb-community@5.0
  ```
  - In Ubuntu:
  ```sh
  sudo systemctl restart mongod
  sudo systemctl status mongod
  ```
- In order to configure mongo's user, open a mongo terminal

```sh
mongosh
rs.initiate() #enabling the replica set successfully
rs.status() #check that replica set is enabled in mongo
use admin
db.createUser(
  {
    user: "root",
    pwd: "Welcome123",
    roles: [
      { role: "userAdminAnyDatabase", db:"admin" },
      { role: "readWriteAnyDatabase", db:"admin" },
      { role: "dbAdminAnyDatabase", db:"admin" },
      { role: "clusterAdmin", db:"admin" }
    ]
  }
)
#check that successfully created
db.getUsers() #fails
db.auth("root","Welcome123")
db.getUsers() #succeeds

use lms
db.createUser(
  {
    user: "napi",
    pwd: "Welcome123",
    roles: [
      { role: "dbOwner", db:"lms" }
    ]
  }
)
use lms_audit
db.createUser(
  {
    user: "napi",
    pwd: "Welcome123",
    roles: [
      { role: "dbOwner", db:"lms_audit" }
    ]
  }
)
use lms_auth
db.createUser(
  {
    user: "napi",
    pwd: "Welcome123",
    roles: [
      { role: "dbOwner", db:"lms_auth" }
    ]
  }
)

#logout with Ctrl + D
```

To mimic production create certificates including the pem file which could be used by mongo

```sh
sudo mkdir -p /opt/data/lms/certs
sudo chown -R `logname`  /opt/data/lms/certs
cd /tmp
curl -O https://raw.githubusercontent.com/nestoru/pob-recipes/master/common/tools/gencert.sh
chmod +x gencert.sh
./gencert.sh US FL Miami app app app app@sample.com
cp app.crt  /opt/data/lms/certs
cp app.key  /opt/data/lms/certs
cp app.crt /opt/data/lms/certs/chain.crt
cat app.key app.crt > /opt/data/lms/certs/app.pem
cp app.crt /opt/data/lms/certs/ca.crt
```

## Required libraries

### weasyprint v57.1

Weasyprint library is used for converting HTML+CSS code to PDF file. Every generation of pdf file requires this library.

[Installation link](https://doc.courtbouillon.org/weasyprint/v57.1/first_steps.html#installation)

## Install all dependencies and run Eslint

To make sure your code is clean, please configure your IDE or run from command line:

```sh
$ cd lms-spa
#Install all dependencies (if you haven't done this previously)
$ npm install
$ npm run eslint

$ cd frontend
#Install all dependencies (if you haven't done this previously)
$ npm install
$ npm run eslint
```

In MAC: if you have some problems with node-gyp install, please check this out https://github.com/nodejs/node-gyp/issues/569

In Linux: If you get an error like `Error: Python executable "/usr/local/bin/python" is v3.6.4, which is not supported by gyp` when installing packages in linux, install python2 then run the below command in order to be able tomake node-gyp work:
`npm config set python /usr/bin/python2.7`

## Tests

Install all dependencies (if you haven't done this previously)

```sh
npm install
```

### With vscode

- Click on the bug on the left side of the screen
- Select `Run Tests` at the top of the side panel
- Click the Green play button.

If it fails, check all the steps mentioned above, you might have to set the environment variables in the `.vscode/launch.json` file.

### Without vscode

Use the following command to run the test from the console. (Remember to change the environment variables value)

```sh
./node_modules/mocha/bin/_mocha ./test/unit/**/*.test.js
```

Or simply use the below if you have setup ~/.bashrc:

```sh
npm run test
```

The test DO NOT need the env vars defined

### Notes on testing

There are two types of testing in this project:

- End to end (e2e), which will test frontend and backend code. It a real frontend app against a real backend app.
- Unit test, that should mock all services (except the one being tested), and make simple assertions of an algorithm.

Each e2e test will create is own testing data and MUST NOT depend on other test nor expect data to exist (except for roles). These test will have to create its own user, its own testing data and ensure that could be run in parallel without affecting other e2e test. In other words, each e2e test MUST BE run in parallel without any issue.

Unit test MUST NOT create persiting data, nor rely on any external service (such as a database or a third party API). These test MUST NOT change any config nor data and SHOULD NOT be executed in production (although there is no mechanism to avoid so at the moment but rather the test folder is not copied).

Integration test have been discouraged in favour of e2e test.

If any unit test or e2e test fail, the application will not be deployed.

### Adhoc Test

The `test` folder contains a subfolder called `adhoc`. You might find some "integration" test there, so why those test are not being taken into account in this project?

The adhoc folder is a place for developers to write tools that helps us to develop a particular feature, for instance:

You are writing a complex query, but you don't want to be testing this from the UI. So you decide to write a script that creates all the data you need in order to test your super cool query. Since you're writing a script that asserts some condition you might choose to write a test.
Then you latter this script (or test) could serve as an informal documentation. This is the purpose of the `adhoc` folder, it contains snippets of code (which could be tests) that developers use as development tools.

As a rule, **only unit test and e2e will determine the project's status**. A failure adhoc script/test **MUST NOT** be considered as any kind of error. No developer is enforced to mantain or update any adhoc script or test.

The latter does not mean that we shouldn't create tools that helps us to do some adhoc testing.

As a convention, every adhoc test data should be created with an additional `__test__data__` property with a true value, like this `{ __test__data__: true }`. Every service should allow any entity to store additional properties (this does not mean that validations should not be performed), so there should be not issue adding this additional property (at least to the "parent" entity such as a `user`).
The generic service class supports the `testData` boolean property. It is setted to false by default but could be replaced to allow the service to automatically set the `__test__data__` property upon creation.

### Adhoc DatabaseTestsuite

For adhoc tests that uses mongo, you can use the `DatabaseTestsuite` class. This class provides some testing goodies and fails without executing if the `NODE_ENV` is not `DEV` or `TEST` (it will fail on production). See `test/adhoc/components/database/mongo/services/grid-service.test.js` to see an example on how to use the `DatabaseTestsuite` class.

When using the `DatabaseTestsuite` you have some test goodies which are really nice.

- Calling the `createUser()` method on an instance it will create an application unique user for each test and destroy it when the test finishes (If you add a `afterEach` function, the user will be destroyed after your `afterEach` function). The user will be available at any time in the test as `this.user`. It is important to create the test as a regular function; arrow functions will break this mechanism and `user` will not be accesible.
- You can skip test by adding calling the skip inner fuction of it, for instance: `it.skip('should skip this test')`.
- If any test hook returns a promise, the instace will wait for this promise resolution to execute its logic.

## Code coverage

### With vscode

- Click on the bug on the left side of the screen
- Select `Coverage` at the top of the side panel
- Click the Green play button.
- Go to the coverage folder and opent the `index.html` file

### Without vscode

Use the following command to run the test from the console. (Remember to change the environment variables value)

```sh
NODE_LOGS_PATH=/var/log/lms/lms-node-log \
AUDIT_TRAIL_FULL_RESPONSE=false \
CRYPTO_KEY_PATH=key/key \
NODE_LOGS_FILTER=password,newPassword,currentPassword,repeatPassword,creditCard,emailDetails \
HTTP_HOST=localhost \
HTTP_PORT=3443 \
export HTTPS_ON=false \
SESSION_TIMEOUT=30 \
LMS_BACKUP_AUDIT_PREFIX=/full/path/to/pts-lms-test-files/backup-audit-files \
LMS_BACKUP_AUDIT_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true" \
LMS_BACKUP_AUDIT_COLLECTIONS=audit_trails \
LMS_BACKUP_NOTIFICATION_PATH=/full/path/to/pts-lms-test-files/backup-notification-files \
LMS_BACKUP_NOTIFICATION_PREFIX=/full/path/to/pts-lms-test-files/backup-notification-files \
LMS_BACKUP_NOTIFICATION_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms?ssl=true" \
LMS_BACKUP_NOTIFICATION_COLLECTIONS=notifications \
LMS_FILES_PATH=/full/path/to/uploads \
LMS_KEY=/opt/data/lms/certs/app.key \
LMS_CRT=/opt/data/lms/certs/app.crt \
AWS_S3_KEY= \
AWS_S3_SECRET= \
AWS_S3_BUCKET= \
GCS_KEY_FILE= \
GCS_BUCKET= \
node ./node_modules/nyc/bin/nyc.js --reporter=html --reporter=text \
./node_modules/mocha/bin/_mocha -u tdd --colors ./test/unit/**/*.test.js
```

## Running the dev server

Install all dependencies (if you haven't done this previously)

```sh
npm install
```

Create a folder and file named "key" for Crypto config

```sh
$ mkdir -p key && echo "dude this is a terrible key" > ./key/key
```

### With vscode

- Click on the bug on the left side of the screen
- Select `Run Server` at the top of the side panel
- Click the Green play button.

If it fails, check all the steps mentioned above, you might have to set the environment variables in the `.vscode/launch.json` file.

### Without vscode

Use the following command to run the server from the console. (Remember to change the environment variables value)

```sh
NODE_LOGS_PATH=/var/log/lms/lms-node-log\
AUDIT_TRAIL_FULL_RESPONSE=false \
CRYPTO_KEY_PATH=key/key \
NODE_LOGS_FILTER=password,newPassword,currentPassword,repeatPassword,creditCard,emailDetails\
HTTP_HOST=localhost\
HTTP_PORT=3443\
export HTTPS_ON=false\
SESSION_TIMEOUT=30\
LMS_BACKUP_AUDIT_PREFIX=/full/path/to/pts-lms-test-files/backup-audit-files \
LMS_BACKUP_AUDIT_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms_audit?ssl=true" \
LMS_BACKUP_AUDIT_COLLECTIONS=audit_trails \
LMS_BACKUP_NOTIFICATION_PATH=/full/path/to/pts-lms-test-files/backup-notification-files \
LMS_BACKUP_NOTIFICATION_PREFIX=/full/path/to/pts-lms-test-files/backup-notification-files \
LMS_BACKUP_NOTIFICATION_CONNECTION_STRING="mongodb://user:password@127.0.0.1:27017/lms?ssl=true" \
LMS_BACKUP_NOTIFICATION_COLLECTIONS=notifications \
LMS_FILES_PATH=/full/path/to/uploads \
LMS_KEY=/opt/data/lms/certs/app.key\
LMS_CRT=/opt/data/lms/certs/app.crt\
ONE_LOGIN_SUBDOMAIN=protranslating-dev\
ONE_LOGIN_ID=\
ONE_LOGIN_SECRET=\
ENABLE_MOCK_LOGIN=\
AWS_S3_KEY= \
AWS_S3_SECRET= \
AWS_S3_BUCKET= \
GCS_KEY_FILE= \
GCS_BUCKET= \
node app/lms.js
```

Or simply use the below if you have setup ~/.bashrc:

```sh
npm run start
```

#### Note

If you have problems with mongo authorization or etc, you can set up it another way.
Change these env vars:

```
LMS_BACKUP_AUDIT_CONNECTION_STRING="mongodb://127.0.0.1:27017/lms_audit"
LMS_BACKUP_NOTIFICATION_CONNECTION_STRING="mongodb://127.0.0.1:27017/lms"
LMS_SECONDARY_CONNECTION_STRING="mongodb://127.0.0.1:27017/lms"
MONGODB_LMS_CONNECTION_STRING=mongodb://127.0.0.1:27017/lms
MONGODB_LMS_AUTH_CONNECTION_STRING=mongodb://127.0.0.1:27017/lms_auth
MONGODB_LMS_AUDIT_CONNECTION_STRING=mongodb://127.0.0.1:27017/lms_audit
```

Then execute:

```
mongod --dbpath /Users/<user>>/Downloads/lms_local_mongo_data
npm run start
```

migrations can fail in that case restart it:

```
mongod --dbpath /Users/<user>>/Downloads/lms_local_mongo_data
```

## Running the frontend app

All the frontend app is self contained in the `frontend` folder. The frontend app requires that the backend application is up and running, all request performed to `/api/*` will be proxied to the backend app. To start the frontend app execute:

```sh
cd frontend
npm install # Install all dependencies (if you haven't done this previously)
npm run dev
```

## Authentication in the local environment

Once you get the server and the frontend running successfully, you're going to need credentials to log into your local version of the application.

Authenticated users can be found in the lmsAuth collection. The password to these accounts can be found by searching the 20210422144317.js migration file.

Note: If you face a recaptcha error while logging into your local app, append mock=true to the url as a query parameter and reload the page.

### Generate the production frontend files

To generate the production files, execute:

```sh
cd frontend
npm install # Install all dependencies (if you haven't done this previously)
npm run build
```

The whole application will now be self contained in the `dist` folder. Upon deploying you can rely that all the production frontend files will be there.

# Migrations

Migrations are node.js script files that run before the application starts. They are located in the `app/migrations` and they are executed only once.

After acquiring a connection with mongo and waiting for migrations to be unlocked (in case some node in a cluster is executing it), the application will proceed to look for non previously executed migrations, it will retrieve all the objects from the `lms_migrations` collection and see if there is any new file that isn't listed there.

Every migration script should be prepared to run as a standalone script or a runnable module, that's why you'll see that most migration scripts will try to acquire a new connection to mongo when in reality, upon running migrations, the connection already exists.

The migration's file name is considered as the migration's name. The migration's file name **MUST** be the migration's UTC creation date with the following format `yyyyMMddHHmmss.js`. Use the `create-migration.sh` script to automatically generate a migration file with this format.

The migration's name (migration file's name) is **CRUCIAL** for the migration execution. If you create a migration called `20170101000000.js` (2017-01-01 00:00:00), the migration will be executed **IF AND ONLY IF** there is no newer migration file name.

If newer migrations appeared during your work, then you can solve this problem using the script `move-migration-top.sh`.
It creates a new file with the most recent timestamp as the filename and with the content from your migration.

example:

```sh
. move-migration-top.sh 20170101000000
```

There is no "tear down" logic. If you make a mistake in a migration and that migration happens to reach production, you'll have to write a new migration that fixes whatever it has to be fixed.

Remember that the application will not start until all new migrations are executed so:

- Keep your migrations short and fast.
- Be sure to not make an infinite loop or something like that.
- Bear in mind that the production environment is likely to have much more than your dev machine, so if you're creating an index consider the posibility of using background index.
- **DO NOT** reuse any migration file that reached master. Most likely they have already run and they won't run again.
- **DO NOT** reuse any migration file that you haven't created.
- **DO NOT** merge migrations. There is a really small chance that a fellow developer creates a migration with the same name as one of yours. You should be **VERY** suspicius when a migration file has been merged, and if you happen to have a conflict in a migration file, backup your changes and **ALWAYS** use the master's version. Again, it's **VERY** unlikely to have two developers pushing two different migrations with the same name.
- If you have to write a long running migration, discuss that with the team. A migration might not be suitable in that case.
- Migrations are not only for production environment but for dev machines as well.
- Migration's execution depends on the `lms_migrations` collection. If there is no data there, all migrations will be executed.
- Before creating a pull request, check if there is no newer migration. Otherwise your migration will not run
- Before making a pull request, always check if there is a newer migration and if it affects any

## Run migrations in standalone mode

`node ./app/run-migrations.js`

# Backups

Folder structure follow year and month to store bson represetantions

```
pts-lms-test-files
├── backup-audit-files
│   └── 2017
│       └── 7
│           └── lms_audit
│               └── audit_trails
└── backup-notification-files
    └── 2017
        └── 7
            └── lms
                └── notifications
```

# Logging

The global application logger is created only once, but a wrapper is created in each request providing the wrapper access to the current request. This is useful in order to append the request's data to a log, more importantly the request's id.
Having the request's id in each log, makes tracking all logs for the same request really easy, and of course it could be merged with the audit trail to fully rebuild request details.

`Label` metadata - label's purpose is similar to request's id which is _to distingiush_ logs related to a specific part of the app(default is not label or `undefined`). In order to take advantage of it use:

```
logger.info('i am a specific label message', { label: 'specific label' });
```

This way the log entry will have a label field.

```
{
  "level": "info",
  "timestamp": "2020-12-30T19:31:13.283Z",
  "message": "i am a specific label message",
  "label": "specific label"
}
```

You can use `jq` to access logs with a specific label.

```
cat logfile.txt | jq 'select(.label=="specific label")'
```

Or to exclude all logs with label

```
cat logfile.txt | jq 'select(.label==null)'
```

## Accessing lms-test logs via Stackdriver

# JIRA deploy logs

Go to https://console.cloud.google.com/logs/query?project=pts-tools and run query for the below:
logName = "projects/pts-tools/logs/deploy-from-jira"

# Release logs

Go to https://console.cloud.google.com/logs/query?project=pts-tools and run query for the below:
logName = "projects/pts-tools/logs/release"

# Test environment Deployment logs

Go to https://console.cloud.google.com/logs/query?project=pts-tools and run query for the below:
logName = "projects/pts-tools/logs/gke-deploy"

# Test environment Apache logs

(logName = "projects/pts-lms-test/logs/stdout" OR logName = "projects/pts-lms-test/logs/stderr")
AND
"lms-apache-test"

# E2E monitoring

Go to https://console.cloud.google.com/logs/query?project=pts-tools and run query for the below (Note that I am adding a second line just to filter the last message printed by the suite that will state if it is failing or not but this is just an example, you can filter by anything you want):
logName = "projects/pts-tools/logs/e2e-cluster"
AND textPayload =~ "Suite"

# Test environment logs

Go to https://console.cloud.google.com/logs/query?project=pts-lms-test and run query for the below:
logName = "projects/pts-lms-test/logs/stdout"

# Prod environment logs

Go to https://console.cloud.google.com/logs/query?project=pts-lms-prod and run query for the below:
(logName = "projects/pts-lms-prod/logs/stdout" OR logName = "projects/pts-lms-prod/logs/stderr")
AND resource.labels.container_name = "lms-nodejs-prod"

# UK environment logs

Go to https://console.cloud.google.com/logs/query?project=pts-lms-uk and run query for the below:
(logName = "projects/pts-lms-uk/logs/stdout" OR logName = "projects/pts-lms-uk/logs/stderr")

# UE environment logs

Go to https://console.cloud.google.com/logs/query?project=pts-lms-ue and run query for the below:
(logName = "projects/pts-lms-ue/logs/stdout" OR logName = "projects/pts-lms-ue/logs/stderr")

# Conventions

- All dates MUST be UTC. The client assumes that all dates sent by the server are in UTC, thus changes from UTC to the local browser timezone.

- In the context of a request, whether you have direct access to the `req` object or not, you should always log with the `req.$logger`. That way the logs are going to be identified with the request and reconstruction of all the request's logs is possible.

- Before creating a pull-request to master (a.k.a deploy) a developer MUST build the production frontend and test your features/bug fixes locally IF frontend has been modified.

- After a developer's pull-request is merged to master, the developer MUST test the features/bug fixes in the testing environment.

- Every database entity could have an `inactive`, so all queries SHOULD add an $or: [{ inactive: false }, { inactive: { $exists: false } }]

- If the Schema uses mongoose-delete plugin, the record in the database will have a filed named 'deleted', so calling model.find will return all records that match 'deleted:false'. Please, go to mongoose-delete docs for more information.

- Every entity saved in the database MUST contain an `created_on` property and `updated_on` property, both UTC dates.

- Every entity saved in the database MUST contain an `created_by` property and `updated_by` both are the emails of the user which created and updated the record. If no user is available (for instance a cron job), the user will ALWAYS be `system@protranslating.com`.

- The application sessions are stored in mongo.

## pm2

In the server side we use pm2. On the development side we could use it as well for testing purposes:

```sh
npm install pm2@latest -g
pm2 start process.yml
```

The process.yaml file must be in the project though and should be ready to be run in the server.

## Prism

Prism was an external service which we are using to query for translation request (temporary). The **referenceNumber** property in the Request schema, is (for now) the former prism Id for that request.

Now you have to encrypt a password with that key, you can use the script located in `test/adhoc/components/crypto/index.js`.

```sh
cd ./test/adhoc/components/crypto/
node index.js
```

That script will print the encrypted password as well as the decryption (just to check that the result is the expected).

Open a mongo client and execute:

```sh
db.users.updateOne({email:'user_to_set_prism@protranslating.com'}, {$set: {'accounts.0.prism.email': 'acmeadmin@sample.com', 'accounts.0.prism.password': 'the_encrypted_string_generated_in_the_command_above'}});
```

Logout and log back in with the user 'user_to_set_prism@protranslating.com'. Remember to have **?noprism=false** feature flag.

## Portal Translator (formerly PortalMT)

Portal Translator is an external service which we are using for translating segmented text. You need it to successfully use `mt-translator/*` endpoints.

```sh
# to connect to the Portal Translator running in the dev server
ssh -L 8000:localhost:8000 ubuntu@35.242.249.218
# to kill
ps -ef|grep uvicorn|grep -v grep|awk '{print $2}'|xargs kill -9
lsof -i:8000
kill -9 {processes}
# to start
cd ~/portalmt/api
nohup uvicorn main:app --reload 2>&1 > /tmp/portalmt.log &
```

#### Notes on testing

There are two types of testing in this project:

- End to end (e2e), which will test frontend and backend code. It is a real frontend app against a real backend app.
- Unit test, that should mock all services (except the one being tested), and make simple assertions of an algorithm.

Each e2e test will create its own testing data and MUST NOT depend on other test nor expect data to exist (except for roles). These tests will have to create its own user, its own testing data and ensure that could be run in parallel without affecting other e2e test. In other words, each e2e test MUST BE run in parallel without any issue.

Unit test MUST NOT create persisting data, nor rely on any external service (such as a database or a third party API). These tests MUST NOT change any config nor data and SHOULD NOT be executed in production (although there is no mechanism to avoid so at the moment but rather the test folder is not copied).

Integration test have been discouraged in favour of e2e test.

If any unit test or e2e test fail, the application will not be deployed.

#### Adhoc test

The `test` folder contains a subfolder called `adhoc`. You might find some "integration" test there, so why those test are not being taken into account in this project?

The adhoc folder is a place for developers to write tools that helps us to develop a particular feature, for instance:

You are writing a complex query, but you don't want to be testing this from the UI. So you decide to write a script that creates all the data you need in order to test your super cool query. Since you're writing a script that asserts some condition you might choose to write a test.
Then later this script (or test) could serve as an informal documentation. This is the purpose of the `adhoc` folder, it contains snippets of code (which could be tests) that developers use as development tools.

As a rule, **only unit test and e2e will determine the project's status**. A failure adhoc script/test **MUST NOT** be considered as any kind of error. No developer is enforced to mantain or update any adhoc script or test.

The latter does not mean that we shouldn't create tools that helps us to do some adhoc testing.

As a convention, every adhoc test data should be created with an additional `__test__data__` property with a true value, like this `{ __test__data__: true }`. Every service should allow any entity to store additional properties (this does not mean that validations should not be performed), so there should be not issue adding this additional property (at least to the "parent" entity such as a `user`).
The generic service class supports the `testData` boolean property. It is set to false by default but could be replaced to allow the service to automatically set the `__test__data__` property upon creation.

##### Adhoc DatabaseTestsuite

For adhoc tests that uses mongo, you can use the `DatabaseTestsuite` class. This class provides some testing goodies and fails without executing if the `NODE_ENV` is not `DEV` or `TEST` (it will fail on production). See `test/adhoc/components/database/mongo/services/grid-service.test.js` to see an example on how to use the `DatabaseTestsuite` class.

When using the `DatabaseTestsuite` you have some test goodies which are really nice.

- Calling the `createUser()` method on an instance it will create an application unique user for each test and destroy it when the test finishes (If you add a `afterEach` function, the user will be destroyed after your `afterEach` function). The user will be available at any time in the test as `this.user`. It is important to create the test as a regular function; arrow functions will break this mechanism and `user` will not be accesible.
- You can skip test by adding calling the skip inner fuction of it, for instance: `it.skip('should skip this test')`.
- If any test hook returns a promise, the instace will wait for this promise resolution to execute its logic.

##### Git Hooks

###### pre-push

Edit file .git/hooks/pre-push with the following content

```sh
#!/bin/bash
set -o errexit
npm run eslint
npm run eslint-frontend
npm test
```

Make it executable

```
chmod +x .git/hooks/pre-push
git config core.hookspath ".git/hooks"
```

## Additional info

- Login to mongo

`mongosh`

- Dumping the database

`mongodump -u napi --db lms --ssl --sslAllowInvalidCertificates`

- Restoring a mongo dump

`mongorestore -u napi --ssl --sslAllowInvalidCertificates --authenticationDatabase lms`

- Wiping all records

```js
db.getCollectionNames().forEach(function (collection) {
  db[collection].remove({});
});
```

- Debug 400 error

Place a breakpoint at line 57 of `node_modules/swagger-tools/middleware/swagger-validator.js`.

- lms-test data and indexes

In lms-test there are some requests that have `no: null`. This triggers the following error:

```sh
E11000 duplicate key error collection: lms.requests index:
lspId_1_no_1 dup key: { : new ObjectId('58f60d08963daf9a13ce1889'), : null }"
```

This is due to this index:

`RequestSchema.index({ lspId: 1, no: 1 }, { unique: true });`

If you are seeing this error in your local environment, please check if you have more than one request with `no: null`

`db.requests.find({no: null}).count();`

## Debugging

```
npm run start-dbg
```

To set a breakpoint use Chrome DevTools as explained in the lms-e2e project but instead of setting an environment var use 'npm run start-dbg'.

Unfortunately at this time the available libraries to perform hot reloading are not mature enough.

## Working with GCS/AWS buckets

## Test AWS S3 credentials

- Install aws cli (https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html). For example at the time of this writing you can use the below:

```
pip3 install awscli --upgrade --user
aws --version

```

- If aws command is not found then add it to the path env:

```
export PATH=$PATH:~/.local/bin

```

- Configure the client and access your bucket (You will need a key and a secret usually shipped in a CSV file):

```
aws configure

```

- List bucket content, copy files etc

```
aws s3 ls s3://pts-lms-local-files-firstname-lastname
echo hello > /tmp/test.txt
aws s3 cp /tmp/test.txt s3://pts-lms-local-files-firstname-lastname/
aws s3 ls s3://pts-lms-local-files-firstname-lastname
```

## Test GCS credentials

- Install gcloud (See "installation instructions" from https://cloud.google.com/sdk/install)
- Authenticate (you will need a key json file). For example:

```
gcloud auth activate-service-account --key-file=/local-path-to-your-downloaded-key-file/pts-lms-local-files-firstname.json

```

- List bucket content, copy files etc

```
gsutil cp /tmp/test.txt gs://pts-lms-local-files-firstname/
gsutil ls gs://pts-lms-local-files-firstname/

```

# Additional Info

## Portal CAT and lms-spa API

Given the fact that this application will be interacting with Portal CAT application we need to match it's API implementation (the routing). Portal CAT has a yaml file that defines it's specification but this application can't use it directly because this API is to expose the UI functionality and not the PC API. The way we build the swagger API docs from this application is by using helper functions (application/definitions) and calling them from the controllers. The API docs are generated automatically but we still need to manually write the API implementation (the routing) for which we will use the Portal CAT yaml file.

## Groups and Roles

Roles are the permissions on objects by subjects within a scope.

`COMPANY_UPDATE_OWN` means that a subject (a user) can update an entity named company only if the user belongs to the company.
Similarly for `COMPANY_UPDATE_ALL` with the exception that the user has the privilege within the whole LSP (all LSP registered companies/clients).

Developers care only about roles, we never code against groups.
Groups consolidate several roles under a name.

For example you can name a group `TEST_GROUP` and add the role `COMPANY_READ_ALL`. When you assign that group to a test user and you login with that test user you should just see the company grid but if you add to the group `USER_READ_ALL` then you will also see the user grid. You wont be able to create or update any of the two entities unless you add additional roles.

A role is therefore comprised of three tokens: `ENTITY_PERMISSION_SCOPE` (row access control) or `ENTITY_PERMISSION_SCOPE` (field access control which is only used as needed really).

## Restoring lms-test backups

1. Download the latest of each of the below mongodb dumps from [pts-e2e-reports](https://console.cloud.google.com/storage/browser/pts-e2e-reports) in the backups folder:

   - lms-test-mongodump-archive-xx.gz

   - lms_auth-test-mongodump-archive-xx.gz

   - epo-mongodump-archive-xx.gz

   - wipo-mongodump-archive-xx.gz

2. Drop all the lms databases and run the following commands in your system terminal:

   - `mongorestore --gzip --archive=/path/to/backups-lms-test-mongodump-archive-xx.gz`

   - `mongorestore --gzip --archive=/path/to/backups-lms_auth-test-mongodump-archive-xx.gz`

   - `mongorestore -d lms -c epo --gzip --archive=/path/to/backups-epo-mongodump-archive-xx.gz`

   - `mongorestore -d lms -c wipo --gzip --archive=/path/to/backups-wipo-mongodump-archive-xx.gz`

Warning: Adding the `--drop` flag to the above commands to drop databases can take a significantly long time. Dropping manually using MongodDB Compass is much faster.

## Running Atlas Snapshot locally

1. Download and unzip an Atlas snapshot
2. Start mongodb instance pointing to the snapshot

```
mongod --dbpath /path/to/snapshot/directory
```

## Working With Transactions in LMS-SPA

`provideTransaction` is the utility method used for working with transactions in the codebase. To work with the `provideTransaction` method:

- Use `provideTransaction` method of your api. It accepts a callback with a single param called session. You will need to pass it into your operatons, f.e.:

```javascript
this.provideTransaction(async (session) => {
  const invoiceDoc = new Invoice(inovice);
  await invoiceDoc.save({ session });
  await someotherstuff({ session });
});
```

## ENTITY NUMBERS RANGE ACROSS THE DEVS

- AG Anton Goroshnikov:
  local: 10000
  devServer: 20000
- AK Alexander Kuzenkov:
  local: 30000
  devServer: 40000
- AM Alejandro Martinez:
  local: 50000
  devServer: 60000
- AP Anton Pashkovskyi:
  local: 70000
  devServer: 80000
- AS Andrey Sukharev:
  local: 90000
  devServer: 100000
- IK Ivan Kozlov:
  local: 130000
  devServer: 140000
- OP Oleg Polyachenkov:
  local: 150000
  devServer: 160000
- MK Masis Karapetyan:
  local: 170000
  devServer: 180000
- VK Valery Khizhevsky:
  local: 190000
  devServer: 200000
- IM Ivan Maurovic:
  local: 210000
  devServer: 220000
- AC Aleksei Chekmenev:
  local: 230000
  devServer: 240000
- RF Reuben Frimpong:
  local: 250000
  devServer: 260000
- AA Alfredo Ajalla:
  local: 270000
  devServer: 280000
- VT Valeria Tomaily:
  local: 290000
  devServer: 300000
- IT Ilnur Timergaliev:
  local: 310000
  devServer: 320000
- SP Sergei Prilutsky :
  local: 330000
  devServer: 340000
- IH Inusah Abdul-Harisu
  local: 350000
  devServer: 360000
- GG Gustavo Guedes :
  local: 370000
  devServer: 380000
- XinJin Li :
  local: 390000
  devServer: 400000
- Romeo Nutifafa Folie:
  local: 410000
  devServer: 420000
- AS Anton Shishenia :
  local: 430000
  devServer: 440000
- Song YiCheng :
  local: 450000
  devServer: 460000
- IF Ilia Fedoseev :
  local: 470000
  devServer: 480000
- JC Jianxing Chao :
  local: 490000
  devServer: 500000
- Adorn Choga:
  local: 510000
  devServer: 520000

# Running the app in cloud servers

## Environment Variables

The env vars on the dev server can be accessed in the `~/bashrc`

## Backend

Start MongoDB instance. Never use sudo for anything other than to install OS packages. Anything like npm, pm2, services like mongo etc must be done with the ubuntu user

- In MacOS:

```sh
brew services start mongodb-community@5.0
```

- In Ubuntu:

```sh
sudo systemctl start mongod
```

Install log-rotate for pm2 to ensure that it's logs never take too much space

```sh
pm2 install pm2-logrotate
pm2 config pm2-logrotate
```

## Running the app with pm2

Open a terminal

Run nano ~/.bashrc or gedit ~/.bashrc

Copy paste the below at the end of the file (replace the path /home/yourname/lms-spa with your app path) and save

alias app='pm2 delete all; cd /home/yourname/lms-spa/frontend; pm2 start -i 1 --name frontend build/dev-server.js --node-args="--max-old-space-size=4096"; cd /home/yourname/lms-spa; pm2 start -i 2 --name backend app/lms.js --node-args="--max-old-space-size=4000"'

Run source ~/.bashrc

Start the app with a simple terminal command

~/> app

# FAQ

- Q: Don't know how to create `connector` collection \ Have local problems with e2e test `accounting-connector`
- A: You need to run ```npm run mock-si-connector``` to create an SI connector for MongoDB.

- Q: How do you fix 'warn: Another instance is running migrations. Waiting for unlocking...'?
- A:
  ```shell
  mongo
  use lms
  db.lms_migration_cluster.find().pretty()
  db.lms_migration_cluster.update({ "flag": "migration_executing" }, { executing: false })
  ```
